# QuokkaQ LLM Configuration
# Copy this file to .env.local and fill in your API keys

# ============================================
# LLM Feature Flags
# ============================================

# Enable/disable LLM integration (set to false to use template-based AI)
NEXT_PUBLIC_USE_LLM=false

# LLM provider to use: "openai" or "anthropic"
NEXT_PUBLIC_LLM_PROVIDER=openai

# ============================================
# OpenAI Configuration
# ============================================

# OpenAI API key (get from: https://platform.openai.com/api-keys)
# WARNING: For demo purposes only! Production should use server-side API keys.
NEXT_PUBLIC_OPENAI_API_KEY=sk-proj-your-openai-api-key-here

# OpenAI model to use (recommended: gpt-4o-mini for cost/speed balance)
NEXT_PUBLIC_OPENAI_MODEL=gpt-4o-mini

# ============================================
# Anthropic Configuration
# ============================================

# Anthropic API key (get from: https://console.anthropic.com/)
# WARNING: For demo purposes only! Production should use server-side API keys.
NEXT_PUBLIC_ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Anthropic model to use (recommended: claude-3-haiku-20240307 for cost/speed)
NEXT_PUBLIC_ANTHROPIC_MODEL=claude-3-haiku-20240307

# ============================================
# LLM Parameters
# ============================================

# Maximum tokens to generate per response
NEXT_PUBLIC_MAX_TOKENS=2000

# Temperature (0.0-1.0): Lower = more focused, Higher = more creative
NEXT_PUBLIC_LLM_TEMPERATURE=0.7

# Top P (0.0-1.0): Nucleus sampling parameter
NEXT_PUBLIC_LLM_TOP_P=0.9

# ============================================
# Cost & Rate Limiting
# ============================================

# Maximum daily cost in USD (triggers alert)
NEXT_PUBLIC_MAX_DAILY_COST=10.00

# Maximum requests per minute (rate limiting)
NEXT_PUBLIC_MAX_REQUESTS_PER_MINUTE=20

# ============================================
# Context Configuration
# ============================================

# Maximum number of course materials to include in context
NEXT_PUBLIC_MAX_CONTEXT_MATERIALS=10

# Minimum relevance score (0-100) for materials to be included
NEXT_PUBLIC_MIN_RELEVANCE_SCORE=30

# Course auto-detection confidence threshold (0-100)
NEXT_PUBLIC_AUTO_DETECT_THRESHOLD=70

# ============================================
# Development Options
# ============================================

# Enable debug logging for LLM calls
NEXT_PUBLIC_DEBUG_LLM=false

# Enable cost tracking display in UI
NEXT_PUBLIC_SHOW_COST_TRACKING=false

# ============================================
# Security Warning
# ============================================
#
# ⚠️ IMPORTANT SECURITY NOTE:
#
# This configuration uses client-side environment variables (NEXT_PUBLIC_*)
# for DEMO PURPOSES ONLY. API keys are visible in the browser.
#
# For production deployment:
# 1. Move API keys to server-side environment variables
# 2. Create API routes that proxy LLM requests
# 3. Implement proper authentication and rate limiting
# 4. Never commit .env.local to version control
#
# See: https://nextjs.org/docs/app/building-your-application/configuring/environment-variables
#
# ============================================
