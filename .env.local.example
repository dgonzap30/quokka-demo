# QuokkaQ Environment Configuration
# Copy this file to .env.local and fill in your values

# ============================================
# SECURITY: API Keys (Server-Side Only)
# ============================================
#
# ✅ SECURE: These keys have NO "NEXT_PUBLIC_" prefix
# They are ONLY accessible in:
# - API routes (app/api/*)
# - Server Components
# - Server Actions
#
# ❌ NOT accessible in browser/client components
#
# Get API keys from:
# - OpenAI: https://platform.openai.com/api-keys
# - Anthropic: https://console.anthropic.com/

OPENAI_API_KEY=sk-proj-your-openai-api-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ============================================
# LLM Feature Flags (Client-Safe)
# ============================================

# Enable/disable LLM integration (set to false to use template-based AI)
NEXT_PUBLIC_USE_LLM=false

# LLM provider to use: "openai" or "anthropic"
NEXT_PUBLIC_LLM_PROVIDER=openai

# ============================================
# Model Configuration (Client-Safe)
# ============================================

# OpenAI model (recommended: gpt-4o-mini for cost/speed balance)
NEXT_PUBLIC_OPENAI_MODEL=gpt-4o-mini

# Anthropic model (recommended: claude-3-haiku-20240307 for cost/speed)
NEXT_PUBLIC_ANTHROPIC_MODEL=claude-3-haiku-20240307

# ============================================
# LLM Parameters (Client-Safe)
# ============================================

# Maximum tokens to generate per response
NEXT_PUBLIC_MAX_TOKENS=2000

# Temperature (0.0-1.0): Lower = more focused, Higher = more creative
NEXT_PUBLIC_LLM_TEMPERATURE=0.7

# Top P (0.0-1.0): Nucleus sampling parameter
NEXT_PUBLIC_LLM_TOP_P=0.9

# ============================================
# Cost & Rate Limiting (Client-Safe)
# ============================================

# Maximum daily cost in USD (triggers alert)
NEXT_PUBLIC_MAX_DAILY_COST=10.00

# Maximum requests per minute (rate limiting)
NEXT_PUBLIC_MAX_REQUESTS_PER_MINUTE=20

# ============================================
# Context Configuration (Client-Safe)
# ============================================

# Maximum number of course materials to include in context
NEXT_PUBLIC_MAX_CONTEXT_MATERIALS=10

# Minimum relevance score (0-100) for materials to be included
NEXT_PUBLIC_MIN_RELEVANCE_SCORE=30

# Course auto-detection confidence threshold (0-100)
NEXT_PUBLIC_AUTO_DETECT_THRESHOLD=70

# ============================================
# Development Options (Client-Safe)
# ============================================

# Enable debug logging for LLM calls
NEXT_PUBLIC_DEBUG_LLM=false

# Enable cost tracking display in UI
NEXT_PUBLIC_SHOW_COST_TRACKING=false

# ============================================
# Backend Integration (Optional)
# ============================================

# Set to true to use the Fastify backend API instead of mock data
NEXT_PUBLIC_USE_BACKEND=false

# Backend API URL (only used if NEXT_PUBLIC_USE_BACKEND=true)
NEXT_PUBLIC_API_URL=http://localhost:3001

# ============================================
# Security Notes
# ============================================
#
# ✅ PRODUCTION-READY CONFIGURATION:
#
# This configuration is secure for production deployment because:
# 1. API keys use server-side env vars (no NEXT_PUBLIC_ prefix)
# 2. API routes handle all LLM calls server-side
# 3. Client never sees or accesses API keys
# 4. Rate limiting is enforced server-side
#
# Additional production recommendations:
# 1. Set API key spending limits in OpenAI/Anthropic dashboards
# 2. Enable rate limiting at the API provider level
# 3. Monitor usage and costs regularly
# 4. Never commit .env.local to version control
#
# See: https://nextjs.org/docs/app/building-your-application/configuring/environment-variables
#
# ============================================
